[{"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/index.js":"1","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/App.js":"2","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/reportWebVitals.js":"3","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Home/Home.js":"4","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Yoga/Yoga.js":"5","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/About/About.js":"6","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Tutorials/Tutorials.js":"7","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/components/Instrctions/Instructions.js":"8","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/components/DropDown/DropDown.js":"9","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/pose_images/index.js":"10","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/data/index.js":"11","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/music/index.js":"12","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/helper/index.js":"13"},{"size":500,"mtime":1722535441000,"results":"14","hashOfConfig":"15"},{"size":496,"mtime":1737787221293,"results":"16","hashOfConfig":"15"},{"size":362,"mtime":1722535441000,"results":"17","hashOfConfig":"15"},{"size":1208,"mtime":1737788086431,"results":"18","hashOfConfig":"15"},{"size":8279,"mtime":1737786607924,"results":"19","hashOfConfig":"15"},{"size":1459,"mtime":1737787831753,"results":"20","hashOfConfig":"15"},{"size":801,"mtime":1737787125351,"results":"21","hashOfConfig":"15"},{"size":838,"mtime":1737788450260,"results":"22","hashOfConfig":"15"},{"size":1161,"mtime":1737786477325,"results":"23","hashOfConfig":"15"},{"size":425,"mtime":1737789514439,"results":"24","hashOfConfig":"15"},{"size":3641,"mtime":1737788462810,"results":"25","hashOfConfig":"15"},{"size":49,"mtime":1722535441000,"results":"26","hashOfConfig":"15"},{"size":351,"mtime":1722535441000,"results":"27","hashOfConfig":"15"},{"filePath":"28","messages":"29","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},"js313",{"filePath":"31","messages":"32","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"33","messages":"34","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"35","messages":"36","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"37","messages":"38","errorCount":0,"fatalErrorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"39","usedDeprecatedRules":"30"},{"filePath":"40","messages":"41","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"42","messages":"43","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"44","messages":"45","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"46","messages":"47","errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"48","usedDeprecatedRules":"30"},{"filePath":"49","messages":"50","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"51","messages":"52","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"53","messages":"54","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},{"filePath":"55","messages":"56","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"30"},"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/index.js",[],["57","58"],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/App.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/reportWebVitals.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Home/Home.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Yoga/Yoga.js",["59","60","61"],"import * as poseDetection from '@tensorflow-models/pose-detection';\nimport * as tf from '@tensorflow/tfjs';\nimport React, { useRef, useState, useEffect } from 'react'\nimport backend from '@tensorflow/tfjs-backend-webgl'\nimport Webcam from 'react-webcam'\nimport { count } from '../../utils/music'; \n \nimport Instructions from '../../components/Instrctions/Instructions';\n\nimport './Yoga.css'\n \nimport DropDown from '../../components/DropDown/DropDown';\nimport { poseImages } from '../../utils/pose_images';\nimport { POINTS, keypointConnections } from '../../utils/data';\nimport { drawPoint, drawSegment } from '../../utils/helper'\n\n\n\nlet skeletonColor = 'rgb(255,255,255)'\nlet poseList = [\n  'Tree', 'Chair', 'Cobra', 'Warrior', 'Dog',\n  'Shoulderstand', 'Traingle'\n]\n\nlet interval\n\n// flag variable is used to help capture the time when AI just detect \n// the pose as correct(probability more than threshold)\nlet flag = false\n\n\nfunction Yoga() {\n  const webcamRef = useRef(null)\n  const canvasRef = useRef(null)\n\n\n  const [startingTime, setStartingTime] = useState(0)\n  const [currentTime, setCurrentTime] = useState(0)\n  const [poseTime, setPoseTime] = useState(0)\n  const [bestPerform, setBestPerform] = useState(0)\n  const [currentPose, setCurrentPose] = useState('Tree')\n  const [isStartPose, setIsStartPose] = useState(false)\n\n  \n  useEffect(() => {\n    const timeDiff = (currentTime - startingTime)/1000\n    if(flag) {\n      setPoseTime(timeDiff)\n    }\n    if((currentTime - startingTime)/1000 > bestPerform) {\n      setBestPerform(timeDiff)\n    }\n  }, [currentTime])\n\n\n  useEffect(() => {\n    setCurrentTime(0)\n    setPoseTime(0)\n    setBestPerform(0)\n  }, [currentPose])\n\n  const CLASS_NO = {\n    Chair: 0,\n    Cobra: 1,\n    Dog: 2,\n    No_Pose: 3,\n    Shoulderstand: 4,\n    Traingle: 5,\n    Tree: 6,\n    Warrior: 7,\n  }\n\n  function get_center_point(landmarks, left_bodypart, right_bodypart) {\n    let left = tf.gather(landmarks, left_bodypart, 1)\n    let right = tf.gather(landmarks, right_bodypart, 1)\n    const center = tf.add(tf.mul(left, 0.5), tf.mul(right, 0.5))\n    return center\n    \n  }\n\n  function get_pose_size(landmarks, torso_size_multiplier=2.5) {\n    let hips_center = get_center_point(landmarks, POINTS.LEFT_HIP, POINTS.RIGHT_HIP)\n    let shoulders_center = get_center_point(landmarks,POINTS.LEFT_SHOULDER, POINTS.RIGHT_SHOULDER)\n    let torso_size = tf.norm(tf.sub(shoulders_center, hips_center))\n    let pose_center_new = get_center_point(landmarks, POINTS.LEFT_HIP, POINTS.RIGHT_HIP)\n    pose_center_new = tf.expandDims(pose_center_new, 1)\n\n    pose_center_new = tf.broadcastTo(pose_center_new,\n        [1, 17, 2]\n      )\n      // return: shape(17,2)\n    let d = tf.gather(tf.sub(landmarks, pose_center_new), 0, 0)\n    let max_dist = tf.max(tf.norm(d,'euclidean', 0))\n\n    // normalize scale\n    let pose_size = tf.maximum(tf.mul(torso_size, torso_size_multiplier), max_dist)\n    return pose_size\n  }\n\n  function normalize_pose_landmarks(landmarks) {\n    let pose_center = get_center_point(landmarks, POINTS.LEFT_HIP, POINTS.RIGHT_HIP)\n    pose_center = tf.expandDims(pose_center, 1)\n    pose_center = tf.broadcastTo(pose_center, \n        [1, 17, 2]\n      )\n    landmarks = tf.sub(landmarks, pose_center)\n\n    let pose_size = get_pose_size(landmarks)\n    landmarks = tf.div(landmarks, pose_size)\n    return landmarks\n  }\n\n  function landmarks_to_embedding(landmarks) {\n    // normalize landmarks 2D\n    landmarks = normalize_pose_landmarks(tf.expandDims(landmarks, 0))\n    let embedding = tf.reshape(landmarks, [1,34])\n    return embedding\n  }\n\n  const runMovenet = async () => {\n    const detectorConfig = {modelType: poseDetection.movenet.modelType.SINGLEPOSE_THUNDER};\n    const detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);\n    const poseClassifier = await tf.loadLayersModel('https://models.s3.jp-tok.cloud-object-storage.appdomain.cloud/model.json')\n    const countAudio = new Audio(count)\n    countAudio.loop = true\n    interval = setInterval(() => { \n        detectPose(detector, poseClassifier, countAudio)\n    }, 100)\n  }\n\n  const detectPose = async (detector, poseClassifier, countAudio) => {\n    if (\n      typeof webcamRef.current !== \"undefined\" &&\n      webcamRef.current !== null &&\n      webcamRef.current.video.readyState === 4\n    ) {\n      let notDetected = 0 \n      const video = webcamRef.current.video\n      const pose = await detector.estimatePoses(video)\n      const ctx = canvasRef.current.getContext('2d')\n      ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);\n      try {\n        const keypoints = pose[0].keypoints \n        let input = keypoints.map((keypoint) => {\n          if(keypoint.score > 0.4) {\n            if(!(keypoint.name === 'left_eye' || keypoint.name === 'right_eye')) {\n              drawPoint(ctx, keypoint.x, keypoint.y, 8, 'rgb(255,255,255)')\n              let connections = keypointConnections[keypoint.name]\n              try {\n                connections.forEach((connection) => {\n                  let conName = connection.toUpperCase()\n                  drawSegment(ctx, [keypoint.x, keypoint.y],\n                      [keypoints[POINTS[conName]].x,\n                       keypoints[POINTS[conName]].y]\n                  , skeletonColor)\n                })\n              } catch(err) {\n\n              }\n              \n            }\n          } else {\n            notDetected += 1\n          } \n          return [keypoint.x, keypoint.y]\n        }) \n        if(notDetected > 4) {\n          skeletonColor = 'rgb(255,255,255)'\n          return\n        }\n        const processedInput = landmarks_to_embedding(input)\n        const classification = poseClassifier.predict(processedInput)\n\n        classification.array().then((data) => {         \n          const classNo = CLASS_NO[currentPose]\n          console.log(data[0][classNo])\n          if(data[0][classNo] > 0.97) {\n            \n            if(!flag) {\n              countAudio.play()\n              setStartingTime(new Date(Date()).getTime())\n              flag = true\n            }\n            setCurrentTime(new Date(Date()).getTime()) \n            skeletonColor = 'rgb(0,255,0)'\n          } else {\n            flag = false\n            skeletonColor = 'rgb(255,255,255)'\n            countAudio.pause()\n            countAudio.currentTime = 0\n          }\n        })\n      } catch(err) {\n        console.log(err)\n      }\n      \n      \n    }\n  }\n\n  function startYoga(){\n    setIsStartPose(true) \n    runMovenet()\n  } \n\n  function stopPose() {\n    setIsStartPose(false)\n    clearInterval(interval)\n  }\n\n    \n\n  if(isStartPose) {\n    return (\n      <div className=\"yoga-container\">\n        <div className=\"performance-container\">\n            <div className=\"pose-performance\">\n              <h4>Pose Time: {poseTime} s</h4>\n            </div>\n            <div className=\"pose-performance\">\n              <h4>Best: {bestPerform} s</h4>\n            </div>\n          </div>\n        <div>\n          \n          <Webcam \n          width='640px'\n          height='480px'\n          id=\"webcam\"\n          ref={webcamRef}\n          style={{\n            position: 'absolute',\n            left: 120,\n            top: 100,\n            padding: '0px',\n          }}\n        />\n          <canvas\n            ref={canvasRef}\n            id=\"my-canvas\"\n            width='640px'\n            height='480px'\n            style={{\n              position: 'absolute',\n              left: 120,\n              top: 100,\n              zIndex: 1\n            }}\n          >\n          </canvas>\n        <div>\n            <img \n              src={poseImages[currentPose]}\n              className=\"pose-img\"\n            />\n          </div>\n         \n        </div>\n        <button\n          onClick={stopPose}\n          className=\"secondary-btn\"    \n        >Stop Pose</button>\n      </div>\n    )\n  }\n\n  return (\n    <div\n      className=\"yoga-container\"\n    >\n      <DropDown\n        poseList={poseList}\n        currentPose={currentPose}\n        setCurrentPose={setCurrentPose}\n      />\n      <Instructions\n          currentPose={currentPose}\n        />\n      <button\n          onClick={startYoga}\n          className=\"secondary-btn\"    \n        >Start Pose</button>\n    </div>\n  )\n}\n\nexport default Yoga","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/About/About.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/pages/Tutorials/Tutorials.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/components/Instrctions/Instructions.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/components/DropDown/DropDown.js",["62"],"import React from 'react'\n\nimport { poseImages } from '../../utils/pose_images'\n\nimport './DropDown.css'\n\nexport default function DropDown({ poseList, currentPose, setCurrentPose }) {\nreturn (\n        <div\n        className='dropdown dropdown-container'\n         \n      >\n        <button \n            className=\"btn btn-secondary dropdown-toggle\"\n            type='button'\n            data-bs-toggle=\"dropdown\"\n            id=\"pose-dropdown-btn\"\n            aria-expanded=\"false\"\n        >{currentPose}\n        </button>\n        <ul class=\"dropdown-menu dropdown-custom-menu\" aria-labelledby=\"dropdownMenuButton1\">\n            {poseList.map((pose) => (\n                <li onClick={() => setCurrentPose(pose)}>\n                    <div class=\"dropdown-item-container\">\n                        <p className=\"dropdown-item-1\">{pose}</p>\n                        <img \n                            src={poseImages[pose]}\n                            className=\"dropdown-img\"\n                        />\n                        \n                    </div>\n                </li>\n            ))}\n            \n        </ul>\n              \n          \n      </div>\n    )\n}\n ","/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/pose_images/index.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/data/index.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/music/index.js",[],"/Users/griffin/Desktop/Github/Assana-Mate/Web/src/utils/helper/index.js",[],{"ruleId":"63","replacedBy":"64"},{"ruleId":"65","replacedBy":"66"},{"ruleId":"67","severity":1,"message":"68","line":4,"column":8,"nodeType":"69","messageId":"70","endLine":4,"endColumn":15},{"ruleId":"71","severity":1,"message":"72","line":53,"column":6,"nodeType":"73","endLine":53,"endColumn":19,"suggestions":"74"},{"ruleId":"75","severity":1,"message":"76","line":252,"column":13,"nodeType":"77","endLine":255,"endColumn":15},{"ruleId":"75","severity":1,"message":"76","line":26,"column":25,"nodeType":"77","endLine":29,"endColumn":27},"no-native-reassign",["78"],"no-negated-in-lhs",["79"],"no-unused-vars","'backend' is defined but never used.","Identifier","unusedVar","react-hooks/exhaustive-deps","React Hook useEffect has missing dependencies: 'bestPerform' and 'startingTime'. Either include them or remove the dependency array.","ArrayExpression",["80"],"jsx-a11y/alt-text","img elements must have an alt prop, either with meaningful text, or an empty string for decorative images.","JSXOpeningElement","no-global-assign","no-unsafe-negation",{"desc":"81","fix":"82"},"Update the dependencies array to be: [bestPerform, currentTime, startingTime]",{"range":"83","text":"84"},[1559,1572],"[bestPerform, currentTime, startingTime]"]